---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- #region id="KcBNv2YLohK9" -->
# Import modules
<!-- #endregion -->

```{python id="YyX5k-wBqY14"}
import datetime
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import time
import warnings

from datetime import date
from itertools import product
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor

warnings.simplefilter('ignore')
```

<!-- #region id="qwqNfi6NQV_v" -->
# Config
<!-- #endregion -->

```{python id="VyDYzlYcQU2M"}
DATA_FILE_PATHS = '/workspace/rahnemacollege/Project/Git/demand-prediction/data/input/'

start_date = '2023-01-01'
start_date_test = '2023-04-01'
end_date = '2023-05-01'

FEATURE_LIST = [
    'PULocationID',
    'PU_day_of_month',
    'PU_day_of_week',
    'last_day_demand',
    'last_week_demand'
]

TARGET = 'count'
VALIDATION_SPLIT_RATIO = 0.2

LR_OUTPUT_PATH = '/workspace/rahnemacollege/Project/Git/demand-prediction/data/output/lr_result.parquet'
XGB_OUTPUT_PATH = '/workspace/rahnemacollege/Project/Git/demand-prediction/data/output/XGB_result.parquet'
```

<!-- #region pycharm={"name": "#%% md\n"} tags=[] -->
# Load data
<!-- #endregion -->

```{python jupyter={'outputs_hidden': False}, pycharm={'name': '#%%\n'}}
def load_data(file_paths, start_date=None, end_date=None):
    df = pd.read_parquet(file_paths)
    df['date'] = df['tpep_pickup_datetime'].dt.date.astype(str)

    if start_date:
        if end_date:
            df = df[(df['date'] >= start_date) & (
                df['date'] < end_date)]
        else:
            df = df[df['date'] > start_date].reset_index(drop=True)
    # Sort the DataFrame based on the 'tpep_pickup_datetime' column in ascending order
    df = df.sort_values(by='date')
    df = df.reset_index(drop=True)

    # Calculate the start time of each interval
    df['interval_start'] = df['tpep_pickup_datetime'].dt.floor('3H')

    # Calculate the end time of each interval
    df['interval_end'] = df['interval_start'] + pd.Timedelta(hours=3)

    # Create a new column with the time interval in the desired format
    df['time_interval'] = df['interval_start'].dt.strftime(
        '%H:%M:%S') + ' - ' + df['interval_end'].dt.strftime('%H:%M:%S')

    # Drop 'interval_start' and 'interval_end' columns
    df.drop(columns=['interval_start', 'interval_end'], inplace=True)

    # Create bins for interval numbers from 1 to 8
    df['time_interval_number'] = pd.cut(
        df['tpep_pickup_datetime'].dt.hour, bins=8, labels=range(1, 9), right=False)

    return df


rides_df = load_data(DATA_FILE_PATHS, start_date, end_date)
print(rides_df.shape)
rides_df.head()
```

<!-- #region tags=[] -->
## Labeling
<!-- #endregion -->

<!-- #region pycharm={"name": "#%% md\n"} tags=[] -->
### Aggregate data and labeling
<!-- #endregion -->

```{python}
def labeling_by_interval(rides_df: pd.DataFrame):
    aggregated_df = rides_df.groupby(
        ['date', 'time_interval_number', 'PULocationID']).size().reset_index(name='count')
    unique_dates = rides_df['date'].unique()
    unique_interval = rides_df['time_interval_number'].unique()
    unique_pu_location_ids = rides_df['PULocationID'].unique()
    all_combinations = list(
        product(unique_dates, unique_interval, unique_pu_location_ids))
    combinations_df = pd.DataFrame(all_combinations, columns=[
                                   'date', 'time_interval_number', 'PULocationID'])
    label_df = aggregated_df.merge(combinations_df, how='right', on=[
                                   'date', 'time_interval_number', 'PULocationID']).fillna(0)
    # Sort based on two columns: 'time_interval_number' (ascending) and 'date' (ascending)
    label_df = label_df.sort_values(
        by=['date', 'time_interval_number'], ascending=[True, True])
    return label_df
```

```{python}
labels_time_df = labeling_by_interval(rides_df)
print(labels_time_df.shape)
labels_time_df.head()
```

<!-- #region tags=[] -->
## Feature Selection
<!-- #endregion -->

### Add calender and previous demands features

```{python}
def adding_feature(rides_df: pd.DataFrame):
    rides_df['date'] = rides_df['date'].astype('datetime64[ns]')
    rides_df['PU_day_of_month'] = rides_df['date'].dt.day.astype(np.uint8)
    rides_df['PU_day_of_week'] = rides_df['date'].dt.weekday.astype(np.uint8)
    rides_df = rides_df.sort_values(
        ['PULocationID', 'date', 'time_interval_number'])
    rides_df['last_interval_demand'] = rides_df.groupby(['PULocationID'])[
        'count'].shift(1)
    rides_df['last_day_demand'] = rides_df.groupby(['PULocationID'])[
        'count'].shift(8)
    rides_df['last_week_demand'] = rides_df.groupby(['PULocationID'])[
        'count'].shift(56)

    # To handle the issue with MAPE and zero values in the count column, we replace zeros
    # with the one value. This prevents division by zero and ensures
    # accurate evaluation of the metric.
    rides_df.loc[rides_df['count']
                 == 0, 'count'] = 1

    return rides_df


labels_time_df_feat = adding_feature(labels_time_df)
print(labels_time_df_feat.shape)
labels_time_df_feat.head()
```

```{python}
rides_df = labels_time_df_feat
```

<!-- #region id="tvzGyWPQEM2-" -->
## Dropping some samples
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 221}, id="VKDnrg9t6u84", outputId="1be04b7c-4beb-4682-c1b9-82dd41db53b3"}
rides_df = rides_df.dropna()
```

<!-- #region id="7wZpKFTMS7Qb" -->
## Train and Test split
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 269}, id="R-OC1_1yS-mF", outputId="5c79c970-8719-4d0d-c154-65ecbf4bdbc7"}
def train_and_test_split(data, split_date):
    train_data = data[
        rides_df['date'] < split_date
    ]
    test_data = data[
        rides_df['date'] >= split_date
    ]

    train_data.set_index('date', inplace=True)
    test_data.set_index('date', inplace=True)

    return train_data, test_data
```

```{python}
train_df, test_df = train_and_test_split(
    rides_df,
    start_date_test
)
```

```{python}
print(train_df.shape)
train_df.head()
```

```{python}
print(test_df.shape)
test_df.head()
```

<!-- #region id="aOdaGdscgNQM" -->
## Target and Feature split
<!-- #endregion -->

```{python id="eoTmtHn-ruLL"}
train_label_df = train_df[TARGET]
train_df = train_df.drop(TARGET, axis=1)

test_label_df = test_df[TARGET]
test_df = test_df.drop(TARGET, axis=1)
```

```{python}
print(train_df.shape)
train_df.head()
```

```{python}
print(train_label_df.shape)
train_label_df.head()
```

```{python}
print(test_df.shape)
test_df.head()
```

```{python}
print(test_label_df.shape)
test_label_df.head()
```

<!-- #region id="0Ohrvwo2fwnC" -->
## Train and Validation split
<!-- #endregion -->

```{python id="A_-X9bYeTO_j"}
train_df, validation_df, train_label_df, validation_label_df = train_test_split(
    train_df,
    train_label_df,
    test_size=VALIDATION_SPLIT_RATIO,
    shuffle=False
)
```

<!-- #region id="ghHG1ei3gdme" -->
# ML Models
<!-- #endregion -->

```{python id="mdnjPVLundY2"}
def model_training(ml_model, train_df, train_label_df, **params):
    model = ml_model(**params)
    model.fit(
        train_df,
        train_label_df
    )
    return model


replace_negatives = np.vectorize(lambda x: 0 if x < 0 else x)
```

<!-- #region id="LN9nCqA9GSy1" -->
## Calculate Error
<!-- #endregion -->

```{python id="wddQ_PcZqlI2"}
def symmetric_mean_absolute_percentage_error(actual, predicted) -> float:
    return round(
        np.mean(
            np.abs(predicted - actual) /
            ((np.abs(predicted) + np.abs(actual)) / 2)
        ), 4
    )


def error_calculator(real_demand, predicted_demand):
    print(
        'SMAPE: ',
        round(
            symmetric_mean_absolute_percentage_error(
                real_demand,
                predicted_demand
            ) * 100, 2
        ), '%'
    )
    print(
        'MAPE:  ',
        round(
            float(
                mean_absolute_percentage_error(
                    real_demand,
                    predicted_demand
                )
            ) * 100, 2
        ), '%'
    )
    print(
        'MSE:   ',
        round(
            float(
                mean_squared_error(
                    real_demand,
                    predicted_demand
                )
            ), 2
        )
    )
    print(
        'MAE:   ',
        round(
            float(
                mean_absolute_error(
                    real_demand,
                    predicted_demand
                )
            ), 2
        )
    )
```

<!-- #region id="UJ9QcWTapixZ" -->
## Linear Regression Model
<!-- #endregion -->

```{python id="P9IrrcU8iAft"}
lr_model = model_training(
    LinearRegression,
    train_df,
    train_label_df
)
```

<!-- #region id="9ioUk22GgpFy" -->
### Validation prediction
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, id="-4qoRLP4VqFr", outputId="d8f41d2a-9566-44cf-957a-c6975f876178"}
lr_validation_pred = replace_negatives(
    np.round_(
        lr_model.predict(
            validation_df
        )
    )
)
error_calculator(
    validation_label_df,
    lr_validation_pred
)
```

<!-- #region id="RtoGP9VchGKZ" -->
### Test prediction
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, id="tt6TaA5SVf65", outputId="8a9741c1-4abd-473c-a721-97c625efa6de"}
lr_test_pred = replace_negatives(
    np.round_(
        lr_model.predict(
            test_df
        )
    )
)
error_calculator(
    test_label_df,
    lr_test_pred
)
```

<!-- #region id="2GZMbrj_4lel" -->
### Result Data
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 252}, id="JvIW0Jme4len", outputId="a0f3234c-ead6-4072-80e9-fd81d38f267f"}
lr_result_df = test_df.copy()
lr_result_df['real demand'] = test_label_df
lr_result_df['predicted demand'] = lr_test_pred

print(lr_result_df.shape)
lr_result_df.head()
```

```{python id="19J1PjyuG-iC"}
lr_result_df.to_parquet(LR_OUTPUT_PATH)
```

<!-- #region id="_Zx1nQT8pixc" -->
## XGBoost Model
<!-- #endregion -->

<!-- #region id="etcdoxu8hcxW" -->
### Hyperparameter tuning
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, id="EtPJikUtoV5t", outputId="23199f57-5128-4836-c235-533e317c4706"}
def hyper_parameter_tuning(n_estimators, learning_rate, max_depth, scoring_method):
    parameters = {
        'n_estimators': n_estimators,
        'learning_rate': learning_rate,
        'max_depth': max_depth
    }

    gc = GridSearchCV(
        XGBRegressor(tree_method='gpu_hist', gpu_id=1),
        parameters,
        scoring=scoring_method
    )

    gc.fit(
        train_df,
        train_label_df
    )

    param = gc.best_params_

    return param


n_estimators = [700, 1000]
learning_rate = [0.15, 0.1, 0.01]
max_depth = [1, 2, 3]
scoring_method = 'neg_root_mean_squared_error'

param = hyper_parameter_tuning(
    n_estimators,
    learning_rate,
    max_depth,
    scoring_method
)

print(param)
```

<!-- #region id="Zo2pKnCThqTm" -->
### XGBoost Model
<!-- #endregion -->

```{python id="4jiwwi53pBbM"}
XGB_model = model_training(
    XGBRegressor,
    train_df,
    train_label_df,
    n_estimators=param['n_estimators'],
    learning_rate=param['learning_rate'],
    max_depth=param['max_depth'],
    tree_method='gpu_hist',
    gpu_id=1
)
```

<!-- #region id="Y1ruHSFikZfu" -->
### Validation prediction
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, id="Cs6kMlFLklAP", outputId="607c7e12-2301-46f7-8c32-f2f8b3b6afcf"}
XGB_validation_pred = replace_negatives(
    np.round_(
        XGB_model.predict(
            validation_df
        )
    )
)
error_calculator(
    validation_label_df,
    XGB_validation_pred
)
```

<!-- #region id="crmdtYCakcDk" -->
### Test prediction
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, id="FTeKWnmNkoWy", outputId="35bf8995-bc54-44f8-da85-f127fe43eaa0"}
XGB_test_pred = replace_negatives(
    np.round_(
        XGB_model.predict(
            test_df
        )
    )
)
error_calculator(
    test_label_df,
    XGB_test_pred
)
```

<!-- #region id="-tvgz0FB4anZ" -->
### Result Data
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 252}, id="gmF1vuTn0l-5", outputId="f17bdc63-1d56-49d5-d862-c7f9d9cbd6a5"}
XGB_result_df = test_df.copy()
XGB_result_df['real demand'] = test_label_df
XGB_result_df['predicted demand'] = XGB_test_pred

print(XGB_result_df.shape)
XGB_result_df.head()
```

```{python id="O0mga6itGpIQ"}
XGB_result_df.to_parquet(XGB_OUTPUT_PATH)
```
